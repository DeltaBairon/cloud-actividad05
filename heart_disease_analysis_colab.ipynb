{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae41e68",
   "metadata": {},
   "source": [
    "# Análisis y Predicción de Enfermedad Cardíaca \n",
    "**Computación en la nube — Actividad 5**\n",
    "\n",
    "**Autor:** Tu Nombre aquí\n",
    "\n",
    "**Fecha:** 2025-10-31 \n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Realizar un análisis completo del *Heart Disease Dataset (UCI)* usando Google Colab como plataforma SaaS y aplicar algoritmos de Machine Learning (Regresión Logística y Random Forest) para predecir la presencia de enfermedad cardíaca.\n",
    "\n",
    "---\n",
    "\n",
    "> Este notebook está escrito en **formato académico**: incluye explicación en Markdown, código comentado, visualizaciones y conclusiones reproducibles en Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d15e7b",
   "metadata": {},
   "source": [
    "## Índice\n",
    "1. [Configuración inicial](#config)\n",
    "2. [Carga de datos](#carga)\n",
    "3. [Exploración de datos (EDA)](#eda)\n",
    "4. [Preprocesamiento](#preproc)\n",
    "5. [Modelado y evaluación](#modelado)\n",
    "6. [Optimización de hiperparámetros](#opt)\n",
    "7. [Conclusiones y recomendaciones](#concl)\n",
    "8. [Referencias](#refs)\n",
    "\n",
    "---\n",
    "\n",
    "<a id='config'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba0388",
   "metadata": {},
   "source": [
    "# 1) Configuración inicial\n",
    "En esta sección instalamos (si es necesario) e importamos las librerías utilizadas. También mostramos cómo montar Google Drive para cargar y guardar archivos.\n",
    "\n",
    "> Nota: En Colab algunas librerías ya vienen instaladas; las líneas `!pip install ...` son opcionales y se ejecutan solo si se necesita una versión específica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías (descomentarlas si las necesitas)\n",
    "# !pip install seaborn plotly xgboost lightgbm\n",
    "\n",
    "# Importación de librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Opcional: montar Google Drive (descomentar en Colab)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185e327",
   "metadata": {},
   "source": [
    "# 2) Carga de datos <a id='carga'></a>\n",
    "\n",
    "Descripción breve:\n",
    "- **Fuente:** UCI — Heart Disease Dataset\n",
    "- **Problema:** Clasificación binaria: presencia (1) o ausencia (0) de enfermedad cardíaca.\n",
    "\n",
    "A continuación cargamos el dataset desde la URL pública (UCI) o desde Google Drive si lo tienes guardado allí.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42009ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset desde UCI (si Colab tiene conexión)\n",
    "# URL alternativa: usar una copia en tu Google Drive si falla\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
    "\n",
    "cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_url, names=cols)\n",
    "    print('Dataset cargado desde UCI. Filas:', df.shape[0])\n",
    "except Exception as e:\n",
    "    print('No se pudo descargar desde UCI. Asegúrate de subir el archivo a Drive y cambiar la ruta. Error:', e)\n",
    "    df = pd.DataFrame()  # placeholder\n",
    "\n",
    "# Vista rápida\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec9ae9",
   "metadata": {},
   "source": [
    "# 3) Exploración de datos (EDA) <a id='eda'></a>\n",
    "\n",
    "En esta sección haremos: descripción, estadísticas, valores faltantes, distribución de variables y visualizaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d48813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información básica\n",
    "print('Dimensiones:', df.shape)\n",
    "print('\\nTipos de datos:\\n', df.dtypes)\n",
    "print('\\nDescripción estadística:')\n",
    "display(df.describe())\n",
    "\n",
    "# Revisar valores únicos y valores faltantes\n",
    "print('\\nValores únicos por columna:')\n",
    "for c in df.columns:\n",
    "    print(f'{c}:', df[c].unique()[:10])\n",
    "    \n",
    "print('\\nValores faltantes por columna:')\n",
    "display(df.isin(['?']).sum())  # en este dataset faltantes a veces representados por '?'\n",
    "display(df.replace('?', np.nan).isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar '?' por NaN y convertir a numérico cuando corresponda\n",
    "df = df.replace('?', np.nan)\n",
    "for c in ['ca','thal']:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Verificar nuevamente\n",
    "display(df.info())\n",
    "display(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ba0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuciones y visualizaciones - al menos 5 visualizaciones\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "# 1. Histograma de edad\n",
    "sns.histplot(df['age'].dropna(), bins=20, kde=True).set_title('Distribución de la edad')\n",
    "\n",
    "# 2. Conteo de target\n",
    "plt.figure(); sns.countplot(x='target', data=df).set_title('Distribución target (0 = no, 1 = yes)')\n",
    "\n",
    "# 3. Correlación\n",
    "plt.figure(figsize=(10,8)); sns.heatmap(df.corr(), annot=True, fmt='.2f').set_title('Matriz de correlación')\n",
    "\n",
    "# 4. Boxplot: colesterol por target\n",
    "plt.figure(); sns.boxplot(x='target', y='chol', data=df).set_title('Colesterol por presencia de enfermedad')\n",
    "\n",
    "# 5. Scatter: maxHR vs age coloreado por target\n",
    "plt.figure(); sns.scatterplot(x='age', y='thalach', hue='target', data=df).set_title('Frecuencia cardiaca máxima vs edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033a6c4",
   "metadata": {},
   "source": [
    "# 4) Preprocesamiento <a id='preproc'></a>\n",
    "\n",
    "Pasos:\n",
    "- Manejo de valores faltantes\n",
    "- Codificación de variables categóricas\n",
    "- Escalado de características\n",
    "- División train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de valores faltantes: estrategia simple (imputación por media/mediana o eliminación cuando hay pocos)\n",
    "print('Valores faltantes antes:')\n",
    "display(df.isna().sum())\n",
    "\n",
    "# Si hay pocas filas con NaN, podemos eliminarlas; sino imputar\n",
    "missing_counts = df.isna().sum()\n",
    "print('\\nColumnas con missing > 0:\\n', missing_counts[missing_counts>0])\n",
    "\n",
    "# Para este notebook haremos imputación simple:\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [float, int] or np.issubdtype(df[col].dtype, np.number):\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print('\\nValores faltantes después:')\n",
    "display(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de variables categóricas (si aplica)\n",
    "# En este dataset algunas variables son categóricas numéricas (cp, restecg, slope, thal)\n",
    "# Convertir target a binario (en el dataset original el target puede ser 0..4 donde >0 indica enfermedad)\n",
    "df['target'] = df['target'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "# Revisar distribución\n",
    "display(df['target'].value_counts())\n",
    "\n",
    "# Selección de características y separación X/y\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Shapes:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5dd15b",
   "metadata": {},
   "source": [
    "# 5) Modelado y evaluación <a id='modelado'></a>\n",
    "\n",
    "Entrenaremos **Regresión Logística** como baseline y **Random Forest**. Evaluaremos con accuracy, precision, recall, f1 y AUC-ROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b299d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_prob_lr = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Logistic Regression - Accuracy:', accuracy_score(y_test, y_pred_lr))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# ROC AUC\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_prob_lr))\n",
    "\n",
    "# 2) Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('\\nRandom Forest - Accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred_rf))\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_prob_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión y curvas ROC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion matrices\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr, cmap='viridis', values_format='d')\n",
    "plt.title('LR - Confusion Matrix')\n",
    "plt.subplot(1,2,2)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='viridis', values_format='d')\n",
    "plt.title('RF - Confusion Matrix')\n",
    "\n",
    "# ROC curves\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'LogReg AUC={roc_auc_score(y_test,y_prob_lr):.2f}')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'RandomForest AUC={roc_auc_score(y_test,y_prob_rf):.2f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.legend(); plt.title('ROC Curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2d62d",
   "metadata": {},
   "source": [
    "# 6) Optimización de hiperparámetros <a id='opt'></a>\n",
    "\n",
    "Usaremos GridSearchCV para optimizar Random Forest (ejemplo simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para RandomForest (ejemplo corto para no tardar demasiado)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=4, scoring='f1', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Mejor parámetro:', grid.best_params_)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Evaluar mejor modelo\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "y_prob_best = best_rf.predict_proba(X_test)[:,1]\n",
    "print('\\nBest RF - Accuracy:', accuracy_score(y_test, y_pred_best))\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_prob_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327660e3",
   "metadata": {},
   "source": [
    "# 7) Conclusiones y recomendaciones <a id='concl'></a>\n",
    "\n",
    "**Resumen de hallazgos**:\n",
    "- Resumen de resultados (compara métricas entre modelos).\n",
    "- Limitaciones: tamaño del dataset, imputación simple, variables categóricas tratadas de forma básica.\n",
    "- Recomendaciones: más datos, validación cruzada más amplia, probar XGBoost/LightGBM, análisis de importancia de variables, explicarabilidad (SHAP).\n",
    "\n",
    "**Reproducibilidad**:\n",
    "- Fijar `random_state` donde aplique.\n",
    "- Documentar versiones de librerías y rutas a datos (colocar copia en Google Drive).\n",
    "\n",
    "---\n",
    "\n",
    "## Archivos a entregar\n",
    "- Notebook `heart_disease_analysis_colab.ipynb`\n",
    "- Carpeta `data/` con dataset original (si se agrega)\n",
    "- `README.md` con instrucciones para reproducir (en el repositorio GitHub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594369b8",
   "metadata": {},
   "source": [
    "# 8) Referencias <a id='refs'></a>\n",
    "- UCI Machine Learning Repository — Heart Disease Dataset\n",
    "- Scikit-learn documentation\n",
    "- Google Colab documentation\n",
    "\n",
    "---\n",
    "\n",
    "**Instrucciones finales:**\n",
    "1. Guarda el notebook y sube a tu Google Drive o GitHub.\n",
    "2. Abre en Google Colab: `File -> Upload notebook` o abrir directamente desde Drive/GitHub.\n",
    "3. Ejecuta todas las celdas (Runtime -> Run all) en Colab.\n",
    "\n",
    "**¡Listo!**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
